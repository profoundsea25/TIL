# [LLM을 활용한 실전 AI 애플리케이션 개발](http://aladin.kr/p/9R7ZW)
- 지은이: 허정준
- 출판사: 책만
- 읽은 날짜: 2025.02. ~ 2025.

## 1부 LLM의 기초 뼈대 세우기
### 1장. LLM 지도
- LLM = 딥러닝 기반 언어 모델
  - LLM은 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩 만들어 가는 방식으로 텍스트를 생성한다. 이러한 모델을 언어 모델이라고 한다.
- 딥러닝이 머신러닝과 가장 큰 차이를 보이는 것 = "데이터 특징을 누가 뽑는가?"
  - 머신러닝 : 데이터의 특징을 연구자/개발자가 찾고 모델에 입력
  - 딥러닝 : 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습
  - 즉, 데이터를 통해 학습하는 과정에서 그 데이터를 가장 잘 이해할 수 있는 방식을 함께 배운다.
- 임베딩
  - 컴퓨터는 숫자만 처리한다. 따라서 딥러닝 모델은 데이터의 의미와 특징을 숫자의 집합으로 표현한다. 이를 임베딩(embedding)이라 부른다.
  - 임베딩은 거리를 계산할 수 있다. 그래서 검색 및 추천, 분류, 이상치 탐지 등을 할 수 있다.
- 언어 모델링
  - 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식
- 전이 학습(transfer learning)
  - 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식
  - 사전 학습 : 대량의 데이터로 모델을 학습
  - 미세 조정(fine-tuning) : 특정한 문제를 해결하기 위한 데이터로 추가 학습
- 정렬(alignment)
  - LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것
- LLM 의 미래
  - 멀티 모달 : 다양한 형태의 입력을 받을 수 있는 LLM (이미지, 오디오, 텍스트 ...)
  - 에이전트 : LLM을 활용해 주어진 상황을 인식하고 필요한 행동을 계획하고 의사결정을 내려 필요한 행동을 직접 수행하는 형태

### 2장. LLM의 중추, 트랜스포머 아키텍처 살펴보기
- 트랜스포머 아키텍처
  - 토큰
    - 거의 모든 자연어 처리 연산의 기본 단위
    - 보통 단어보다 짧은 텍스트 단위
  - 토큰화
    - 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것
  - RNN(순환신경망)
    - 순차적 처리
    - 깊이가 깊어지면 문제가 발생함
  - 셀프 어텐션
    - 순차 처리 방식을 버리고, 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산하여 각 단어의 표현(representation)을 조정
  - 사람이 글을 이해하는 것처럼 딥러닝 모델이 작동하도록 하려면 단어 사이의 관계를 계산해 관련이 있는지 찾고,관련이 있는 단어의 맥락을 포함시켜 단어를 재해석해야 한다.
    - 단어 사이의 관련성을 파악하기 위해 토큰 임베딩에 가중치를 도입한다. 이를 통해 내부적으로 토큰과 토큰 사이의 관계를 계산해서 적절히 주변 맥락을 반영하는 방법을 학습한다.
  - 인코더와 디코더로 구성
    - 인코더 집중 모델 : 구글 BERT
      - 자연어 이해에 강점
    - 디코더 집중 모델 : OpenAI GPT
      - 자연어 생성에 강점
    - 인코더-디코더 활용 모델 : 메타 BART, 구글 T5
      - 자연어 이해, 생성을 모두 활용하지만 매우 모델이 복잡하다.
- 주요 사전 학습 매커니즘
  - 인과적 언어 모델링
    - 문장의 시작부터 끝까지 순차적으로 단어를 예측
    - 이전에 등장한 단어들을 바탕으로 다음에 등장할 단어 예측
    - GPT 같은 생성 트랜스포머 모델에서 핵심적인 학습 방법
  - 마스크 언어 모델링
    - 입력 단어의 일부를 마스크 처리하고 그 단어를 맞추는 작업으로 모델을 학습

## 3장. 트랜스포머 모델을 다루기 위한 허길페이스 트랜스포머 라이브러리
